{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c1e5db-b21e-4588-b942-3c76634e63b1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(20, 20, 20, 0.8); color:white; padding:10pt\" >    \n",
    "\n",
    "\n",
    "# **TP Apprentissage par renforcement**\n",
    "&mdash; **`alain.lebret@ensicaen.fr`** &mdash;\n",
    "\n",
    "### Algorithme Q-Learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1c974-6bde-450a-8a86-c904674a4c88",
   "metadata": {},
   "source": [
    "## Objectif\n",
    "\n",
    "Le but de ce TP est de vous familiariser avec l'apprentissage automatique et notamment l'apprentissage par **renforcement**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e04e34-a055-430a-b9f5-a6e18a30b5fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dépendances\n",
    "\n",
    "Avant de commencer ce TP, assurez-vous d'avoir installé les bibliothèques Python suivantes : *NumPy* et *Matplotlib* (c'est en principe le cas).\n",
    "\n",
    "#### Bibliothèques utilisées\n",
    "\n",
    "- **NumPy** ([documentation](https://www.numpy.org/)) : *NumPy* est une bibliothèque de calcul matriciel largement utilisée pour ses fonctionnalités en algèbre linéaire, transformation de Fourier, et gestion des nombres aléatoires.\n",
    "- **Matplotlib** ([documentation](https://matplotlib.org/)) : *Matplotlib* est une bibliothèque de visualisation en Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16f998e-fc6c-4373-83b5-33a2f8372766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ffd70-fd4e-48ed-a221-4d548cf952eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "L'apprentissage par **renforcement** est une méthode d'apprentissage automatique qui permet à des programmes informatiques, que nous appelons des \"agents\", d'apprendre à prendre des décisions optimales à travers des expériences. Imaginez un robot qui apprend à naviguer dans un labyrinthe ; à chaque intersection, il doit choisir entre tourner à gauche, à droite ou continuer tout droit. L'objectif est de trouver la sortie le plus rapidement possible. L'apprentissage par renforcement aide le robot à déterminer la meilleure action à prendre à chaque étape en fonction de sa position actuelle et des expériences passées.\n",
    "\n",
    "#### Éléments de base\n",
    "\n",
    "- L'**agent** : C'est l'entité qui apprend à accomplir une tâche. Par exemple, un bras robotique qui apprend à déplacer des objets, un robot sommelier qui propose le vin à associer à un plat, une voiture autonome qui ménage sa batterie, un agent conversationnel qui cherche à satisfaire la demande d'un client, etc.\n",
    "- L'**environnement** : C'est le monde dans lequel l'agent opère. Pour un capteur dans une centrale nucléaire, l'environnement serait la centrale elle-même et les données qu'il mesure.\n",
    "- Les **états** : Ce sont les différentes situations ou configurations possibles de l'environnement. Pour un capteur, un état pourrait être la lecture actuelle de la température ou de la pression.\n",
    "- Les **actions** : Ce sont les différentes opérations que l'agent peut effectuer. Dans le cas de la robotique, cela pourrait être le déplacement d'un bras robotique ou l'ajustement d'un paramètre sur un capteur.\n",
    "- Les **récompenses** : Après chaque action, l'agent reçoit une \"récompense\" ou une \"punition\" qui lui indique si l'action était bénéfique ou non. Un capteur qui détecte un risque dans la centrale pourrait déclencher une action qui prévient un accident, ce qui serait une grande récompense.\n",
    "\n",
    "L'agent essaie différentes actions et apprend de ses erreurs et de ses succès. Au début, ses choix sont aléatoires, mais au fil du temps, il commence à reconnaître les chemins qui mènent à de meilleures récompenses.\n",
    "\n",
    "Parmi les différents algorithmes d'apprentissage par renforcement, on trouve l'algorithme *Q-learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f34586",
   "metadata": {},
   "source": [
    "## 1. Mise en oeuvre de l'algorithme Q-learning\n",
    "\n",
    "L'algorithme *Q-learning* permet à un agent d'apprendre à prendre des décisions en calculant ce que l'on appelle une \"table de qualité\" (*Q-table* en anglais). Cette table aide l'agent à estimer quelles actions sont les plus prometteuses en termes de récompenses futures.\n",
    "Le *Q-learning* est un algorithme dit \"sans modèle\", ce qui signifie qu'il n'a pas besoin de comprendre ou de modéliser l'environnement dans lequel il opère. Il apprend uniquement à partir des récompenses qu'il reçoit en conséquence de ses actions. C'est un peu comme si vous appreniez à trouver votre chemin dans une ville étrangère sans carte : au lieu de cela, vous vous souvenez des rues qui vous ont mené à des destinations agréables et vous évitez celles qui vous ont conduit dans des impasses.\n",
    "\n",
    "Le *Q-learning* est particulièrement utile dans les situations où l'environnement est complexe et changeant, comme dans la gestion des opérations d'une centrale nucléaire, dans la navigation d'un robot dans un espace encombré ou à haut risque, et pourquoi pas d'un robot sommelier. Il permet à l'agent d'apprendre de ses erreurs et de s'adapter à de nouvelles situations sans avoir besoin de reprogrammation.\n",
    "\n",
    "#### Comment fonctionne le Q-learning ?\n",
    "\n",
    "La *Q-table* est un tableau qui indique la récompense attendue de chaque action dans chaque état. Chaque ligne de la table correspond à un état possible dans l'environnement, et chaque colonne correspond à une action possible que l'agent peut prendre.\n",
    "Voici à quoi ressemble une *Q-table* pour un environnement simple avec 3 états et 2 actions possibles :\n",
    "\n",
    "| États/Actions |  Action 1 |  Action 2 |\n",
    "|---------------|-----------|-----------|\n",
    "| État 1        | Q(e1, a1) | Q(e1, a2) |\n",
    "| État 2        | Q(e2, a1) | Q(e2, a2) |\n",
    "| État 3        | Q(e3, a1) | Q(e3, a2) |\n",
    "\n",
    "- `Q(e1, a1)` est la valeur de la récompense si l'on choisit l'Action 1 lorsqu'on est dans l'État 1.\n",
    "- `Q(e2, a2)` est la valeur de la récompense si l'on choisit l'Action 2 lorsqu'on est dans l'État 2, et ainsi de suite.\n",
    "\n",
    "L'algorithme est le suivant :\n",
    "\n",
    "1. **Initialisation** : On commence par créer la *Q-table* avec des valeurs initiales arbitraires. \n",
    "2. **Exploration** : L'agent explore l'environnement et, à chaque étape, met à jour la *Q-table* en fonction des récompenses obtenues. Au début, l'agent essaie des actions au hasard pour collecter des informations sur l'environnement.\n",
    "3. **Exploitation** : Avec le temps, l'agent commence à utiliser les informations accumulées dans la *Q-table* pour choisir les actions qui semblent offrir les meilleures récompenses futures.\n",
    "4. **Mise à jour de la Q-table** : Après chaque action, l'agent met à jour la *Q-table* en utilisant la formule de Bellman, qui intègre la récompense reçue et les récompenses futures estimées :\n",
    "    $Q_{\\text{nouveau}}(e, a) = Q(e, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(e', a') - Q(e, a) \\right]$\n",
    "\n",
    "    où :\n",
    "    - $Q(e, a)$ est la valeur actuelle de la *Q-table* pour l'état actuel $e$ et l'action $a$.\n",
    "    - $Q_{\\text{nouveau}}(e, a)$ est la nouvelle valeur mise à jour pour cet état et cette action.\n",
    "    - $\\alpha$ est le taux d'apprentissage, qui détermine à quel point les nouvelles informations influencent les anciennes. Une valeur plus élevée permet à l'agent d'apprendre plus rapidement, mais une valeur trop élevée peut rendre l'apprentissage instable. \n",
    "    - $r$ est la récompense reçue après avoir exécuté l'action $a$.\n",
    "    - $\\gamma$ est le facteur d'actualisation qui évalue l'importance des récompenses futures. Une valeur proche de 1 fera en sorte que l'agent valorise fortement les récompenses à long terme, alors que pour une valeur proche de 0 l'agent valorise les récompenses immédiates.\n",
    "    - $\\max_{a'} Q(e', a')$ est la valeur maximale de $Q$ pour le prochain état $e'$, en prenant en compte toutes les actions possibles $a'$. Cela représente la meilleure récompense future espérée.\n",
    "\n",
    "> Remarque : vous pourrez avoir besoin de la fonction [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) pour récupérer $\\max_{a'} Q(e', a')$.\n",
    "\n",
    "#### Stratégie $\\epsilon$-greedy\n",
    "\n",
    "La stratégie $\\epsilon$-greedy consiste à aider un algorithme à décider quand il est nécessaire d'essayer de nouvelles actions ou d'utiliser ce qu'il sait déjà. Autrement dit, c'est une stratégie qui va aider à choisir entre **exploration** de l'environnement et **exploitation** de celui-ci. Dans cette stratégie, le taux d'exploration $\\epsilon$ est un nombre compris entre 0 et 1 qui peut varier avec le temps, notamment dans le cas d'un environnement dynamique.\n",
    "\n",
    "L'algorithme est le suivant :\n",
    "\n",
    "```\n",
    "Si un nombre aléatoire est inférieur à epsilon :\n",
    "    Choisir une action aléatoire parmi toutes les actions possibles. (Exploration)\n",
    "Sinon :\n",
    "    Choisir l'action qui a la plus haute valeur Q pour l'état actuel. (Exploitation)\n",
    "```\n",
    "\n",
    "Dans sa version de base, les actions testées sont choisies au hasard, sans penser à leur potentiel, ce qui est un inconvénient.\n",
    "\n",
    "#### Une première mise en oeuvre\n",
    "\n",
    "Voici deux premiers exercices qui n'ont pour seul intérêt que de vous entraîner à définir des états, des actions, une *Q-table* et de mettre en oeuvre la formule de Bellman afin de mettre à jour la table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd727363-fe24-44e9-afc1-2967fe3cab98",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 183, 77, 0.6); padding:5pt\" >    \n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercice n° 1.1**\n",
    "\n",
    "Imaginons un jeu où il y a trois gobelets retournés : 'A', 'B' et 'C'. Sous un de ces gobelets se trouve une récompense. Le but est de choisir le bon gobelet pour obtenir cette récompense. Au début, vous n'avez aucune idée du gobelet à choisir, mais avec le temps, vous pouvez apprendre quel gobelet a la plus grande probabilité de contenir la récompense.\n",
    "\n",
    "##### Environnement\n",
    "- **États** : Il n'y a qu'un seul état dans cet environnement, car il n'y a pas de séquence d'actions à prendre en compte.\n",
    "- **Actions** : Les actions possibles consistent à choisir le gobelet 'A', 'B' ou 'C'.\n",
    "- **Récompenses** : La récompense est de +1 si vous choisissez le bon gobelet, et 0 sinon.\n",
    "\n",
    "##### Règles\n",
    "Après chaque choix, le jeu vous informe si vous avez choisi le bon gobelet ou non.\n",
    "La position de la récompense ne change pas pendant une \"session\" d'apprentissage, mais peut changer entre les sessions afin d'éviter que l'agent n'apprenne simplement à toujours choisir le même gobelet.\n",
    "\n",
    "L'objectif est de coder un agent qui utilise le *Q-learning* pour déterminer le meilleur gobelet à choisir pour maximiser ses récompenses sur le long terme.\n",
    "\n",
    "##### 1. Initialisation\n",
    "Comme il n'y a qu'un seul état, la *Q-table* ne comporte qu'une seule ligne que nous appelerons `Q_values`. Nous la représentons par un dictionnaire dont les clés (actions) sont les noms des gobelets ('A', 'B' ou 'C') et les valeurs sont au départ à 0.\n",
    "Le gobelet contenant la récompense est choisi aléatoirement à l'aide de la fonction `np.random.choice()`. Le taux d'exploration est fixé à 0.1, le taux d'apprentissage à 0.1 et le nombre d'épisodes à 100. \n",
    "\n",
    "##### 2. Choix d'action\n",
    "Compléter la fonction `choose_action()` qui prend comme paramètre la table `Q_values` et `epsilon`, et qui applique une stratégie $\\epsilon$-greedy. La fonction retourne une action aléatoire ou celle qui présente la plus grande valeur dans `Q_values`.\n",
    "\n",
    "##### 3. Calcul des récompenses\n",
    "Compléter la fonction `get_reward()` qui à chaque tour prend comme paramètre l'action retournée par `choose_action()` et le nom du gobelet gagnant. La fonction retourne 1 si le gobelet choisi correspond au gobelet gagnant, 0 sinon.\n",
    "\n",
    "##### 4. Mise à jour des valeurs Q\n",
    "Compléter la fonction `update_Q_values()` qui met à jour la valeur Q pour l'action choisie en utilisant la récompense reçue. Cette fonction réalise la formule de Bellman dans laquelle nous considérerons que $\\gamma$ est nul.\n",
    "\n",
    "\n",
    "L'entraînement est répété 100 fois afin d'apprendre quel gobelet a la plus haute valeur Q, c'est-à-dire la plus grande chance d'offrir une récompense.\n",
    "\n",
    "---\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c85ca6-5582-433f-91d8-87346eb9eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "Valeurs Q :  {'A': np.float64(0.0), 'B': np.float64(0.19), 'C': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "# Initialisation des paramètres du Q-learning\n",
    "Q_values = {'A': 0, 'B': 0, 'C': 0}  # Valeurs Q pour chaque action\n",
    "reward_goblet = np.random.choice(['A', 'B', 'C'])  # Gobelet avec récompense\n",
    "print(reward_goblet)\n",
    "epsilon = 0.1  # Taux d'exploration\n",
    "alpha = 0.1    # Taux d'apprentissage\n",
    "episodes = 100\n",
    "gamma = 0.99\n",
    "# Nombre d'épisodes pour l'apprentissage\n",
    "\n",
    "# Fonction pour choisir une action\n",
    "def choose_action(Q_values, epsilon):\n",
    "    choice = np.random.rand()\n",
    "    if choice <= epsilon: # exploration\n",
    "        chosen_goblet = np.random.choice(list(Q_values.keys()))\n",
    "    else :\n",
    "        chosen_goblet = list(Q_values.keys())[np.argmax(Q_values.values())]\n",
    "    # À compléter pour choisir une action en utilisant epsilon-greedy\n",
    "    return chosen_goblet\n",
    "\n",
    "# Fonction pour obtenir une récompense\n",
    "def get_reward(action, reward_goblet):\n",
    "    reward = 0\n",
    "    if reward_goblet == action:\n",
    "        reward = 1\n",
    "    return reward\n",
    "\n",
    "# Fonction pour mettre à jour les valeurs Q en fonction de l'action, la réaction et la valeur de alpha\n",
    "def update_Q_values(Q_values, action, reward, alpha):\n",
    "    Q_values[action]  = Q_values[action] + alpha * (reward + gamma * np.argmax(Q_values[action]) - Q_values[action])\n",
    "\n",
    "# Processus d'apprentissage\n",
    "for episode in range(episodes):\n",
    "    action = choose_action(Q_values, epsilon)\n",
    "    reward = get_reward(action, reward_goblet)\n",
    "    update_Q_values(Q_values, action, reward, alpha)\n",
    "\n",
    "# Affichage des valeurs Q apprises\n",
    "print(\"Valeurs Q : \", Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ee834-780d-43fe-9f98-5338c0c2a4dc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 183, 77, 0.6); padding:5pt\" >    \n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercice n° 1.2**\n",
    "\n",
    "Imaginons un robot se déplaçant dans un couloir avec 5 positions possibles (0 à 4), où 0 est le point de départ et 4 est le point d'arrivée avec une récompense.\n",
    "\n",
    "##### Environnement\n",
    "- **États** : Ce sont les positions dans le couloir (de 0 à 4).\n",
    "- **Actions** : Les actions possibles consistent à aller à gauche (0) ou à droite (1), sauf aux extrémités du couloir.\n",
    "- **Récompenses** : La récompense est de +1 si le robot atteint la position 4, et 0 sinon.\n",
    "\n",
    "##### 1. Initialisation\n",
    "La *Q-table* est une matrice de 5 lignes (5 états pour 5 positions dans le couloir) et 2 colonnes (2 actions, 0 pour 'gauche' et 1 pour 'droite'). Le taux d'apprentissage est fixé à 0.1, le facteur de remise à 0.9 et le taux d'exploration à 0.1.\n",
    "\n",
    "##### 2. Choix d'action\n",
    "Compléter la fonction `choose_action()` qui prend comme paramètre l'état, et qui applique une stratégie $\\epsilon$-greedy. La fonction retourne une action aléatoire ou celle qui présente la plus grande valeur dans `Q_table`.\n",
    "\n",
    "##### 3. Déterminer le nouvel état\n",
    "Compléter la fonction `get_next_state()` qui prend comme paramètre l'action à réaliser et retourne le nouvel état. Cette fonction doit s'assurer que le nouvel état ne dépasse pas les limites.\n",
    "\n",
    "##### 3. Calcul des récompenses\n",
    "Compléter la fonction `get_reward()` qui prend comme paramètre le nouvel état retourné par `get_next_state()`. La fonction retourne 1 si l'état correspond à la position 4, 0 sinon.\n",
    "\n",
    "##### 4. Mise à jour des valeurs Q\n",
    "Compléter la fonction `update_Q_table()` qui met à jour la valeur Q pour l'action choisie en utilisant la récompense reçue. Cette fonction réalise la formule de Bellman.\n",
    "\n",
    "\n",
    "L'apprentissage est répété 100 fois.\n",
    "\n",
    "---\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19be29a5-7266-4b29-b9a8-7729fbd6637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0\n",
      "[[0.  0. ]\n",
      " [0.  0. ]\n",
      " [0.  0. ]\n",
      " [0.  0.1]\n",
      " [0.  0. ]]\n",
      " epoch 1\n",
      "[[0.    0.   ]\n",
      " [0.    0.   ]\n",
      " [0.    0.009]\n",
      " [0.    0.19 ]\n",
      " [0.    0.   ]]\n",
      " epoch 2\n",
      "[[0.      0.     ]\n",
      " [0.      0.00081]\n",
      " [0.      0.0252 ]\n",
      " [0.      0.271  ]\n",
      " [0.      0.     ]]\n",
      " epoch 3\n",
      "[[0.000e+00 7.290e-05]\n",
      " [0.000e+00 2.997e-03]\n",
      " [0.000e+00 4.707e-02]\n",
      " [0.000e+00 3.439e-01]\n",
      " [0.000e+00 0.000e+00]]\n",
      " epoch 4\n",
      "[[0.0000e+00 3.3534e-04]\n",
      " [0.0000e+00 6.9336e-03]\n",
      " [0.0000e+00 7.3314e-02]\n",
      " [0.0000e+00 4.0951e-01]\n",
      " [0.0000e+00 0.0000e+00]]\n",
      " epoch 5\n",
      "[[0.         0.00092583]\n",
      " [0.         0.0128385 ]\n",
      " [0.         0.12941055]\n",
      " [0.00925547 0.468559  ]\n",
      " [0.         0.        ]]\n",
      " epoch 6\n",
      "[[0.         0.00198871]\n",
      " [0.         0.0232016 ]\n",
      " [0.         0.15863981]\n",
      " [0.00925547 0.5217031 ]\n",
      " [0.         0.        ]]\n",
      " epoch 7\n",
      "[[0.         0.00387798]\n",
      " [0.         0.03515902]\n",
      " [0.         0.1897291 ]\n",
      " [0.00925547 0.56953279]\n",
      " [0.         0.        ]]\n",
      " epoch 8\n",
      "[[0.         0.0066545 ]\n",
      " [0.         0.04871874]\n",
      " [0.         0.22201414]\n",
      " [0.00925547 0.61257951]\n",
      " [0.         0.        ]]\n",
      " epoch 9\n",
      "[[0.         0.01037373]\n",
      " [0.         0.06382814]\n",
      " [0.         0.25494489]\n",
      " [0.00925547 0.65132156]\n",
      " [0.         0.        ]]\n",
      " epoch 10\n",
      "[[0.         0.01931734]\n",
      " [0.00135728 0.08039036]\n",
      " [0.         0.28806934]\n",
      " [0.00925547 0.6861894 ]\n",
      " [0.         0.        ]]\n",
      " epoch 11\n",
      "[[0.00173856 0.02462074]\n",
      " [0.00135728 0.09827757]\n",
      " [0.         0.32101945]\n",
      " [0.00925547 0.71757046]\n",
      " [0.         0.        ]]\n",
      " epoch 12\n",
      "[[0.00173856 0.03100364]\n",
      " [0.00135728 0.11734156]\n",
      " [0.         0.35349885]\n",
      " [0.00925547 0.74581342]\n",
      " [0.         0.        ]]\n",
      " epoch 13\n",
      "[[0.00173856 0.03846402]\n",
      " [0.00135728 0.1374223 ]\n",
      " [0.         0.38527217]\n",
      " [0.00925547 0.77123208]\n",
      " [0.         0.        ]]\n",
      " epoch 14\n",
      "[[0.00173856 0.04698562]\n",
      " [0.00135728 0.15835457]\n",
      " [0.         0.41615584]\n",
      " [0.00925547 0.79410887]\n",
      " [0.         0.        ]]\n",
      " epoch 15\n",
      "[[0.00173856 0.05653897]\n",
      " [0.00135728 0.17997314]\n",
      " [0.         0.44601005]\n",
      " [0.00925547 0.81469798]\n",
      " [0.         0.        ]]\n",
      " epoch 16\n",
      "[[0.00173856 0.06708266]\n",
      " [0.00135728 0.20211673]\n",
      " [0.         0.47473187]\n",
      " [0.00925547 0.83322818]\n",
      " [0.         0.        ]]\n",
      " epoch 17\n",
      "[[0.00173856 0.0785649 ]\n",
      " [0.00135728 0.22463092]\n",
      " [0.         0.52701483]\n",
      " [0.05353235 0.84990536]\n",
      " [0.         0.        ]]\n",
      " epoch 18\n",
      "[[0.00173856 0.09092519]\n",
      " [0.00135728 0.24959916]\n",
      " [0.         0.55080483]\n",
      " [0.05353235 0.86491483]\n",
      " [0.         0.        ]]\n",
      " epoch 19\n",
      "[[0.00173856 0.1042966 ]\n",
      " [0.00135728 0.27421168]\n",
      " [0.         0.57356668]\n",
      " [0.05353235 0.87842335]\n",
      " [0.         0.        ]]\n",
      " epoch 20\n",
      "[[0.00173856 0.11854599]\n",
      " [0.00135728 0.29841152]\n",
      " [0.         0.59526812]\n",
      " [0.05353235 0.89058101]\n",
      " [0.         0.        ]]\n",
      " epoch 21\n",
      "[[0.00173856 0.14918659]\n",
      " [0.01324091 0.36272789]\n",
      " [0.05700908 0.61589359]\n",
      " [0.05353235 0.90152291]\n",
      " [0.         0.        ]]\n",
      " epoch 22\n",
      "[[0.00173856 0.16691344]\n",
      " [0.01324091 0.38188552]\n",
      " [0.05700908 0.6354413 ]\n",
      " [0.05353235 0.91137062]\n",
      " [0.         0.        ]]\n",
      " epoch 23\n",
      "[[0.00173856 0.18459179]\n",
      " [0.01324091 0.40088669]\n",
      " [0.05700908 0.65392052]\n",
      " [0.05353235 0.92023356]\n",
      " [0.         0.        ]]\n",
      " epoch 24\n",
      "[[0.00173856 0.20221242]\n",
      " [0.01324091 0.41965087]\n",
      " [0.05700908 0.67134949]\n",
      " [0.05353235 0.9282102 ]\n",
      " [0.         0.        ]]\n",
      " epoch 25\n",
      "[[0.00173856 0.21975975]\n",
      " [0.01324091 0.43810723]\n",
      " [0.05700908 0.68775346]\n",
      " [0.05353235 0.93538918]\n",
      " [0.         0.        ]]\n",
      " epoch 26\n",
      "[[0.00173856 0.23721343]\n",
      " [0.01324091 0.45619432]\n",
      " [0.05700908 0.70316314]\n",
      " [0.05353235 0.94185026]\n",
      " [0.         0.        ]]\n",
      " epoch 27\n",
      "[[0.00173856 0.27015211]\n",
      " [0.03482628 0.47385957]\n",
      " [0.05700908 0.71761335]\n",
      " [0.05353235 0.94766524]\n",
      " [0.         0.        ]]\n",
      " epoch 28\n",
      "[[0.00173856 0.28578426]\n",
      " [0.03482628 0.49105882]\n",
      " [0.05700908 0.73114189]\n",
      " [0.05353235 0.95289871]\n",
      " [0.         0.        ]]\n",
      " epoch 29\n",
      "[[0.00173856 0.30140112]\n",
      " [0.03482628 0.5077557 ]\n",
      " [0.05700908 0.74378858]\n",
      " [0.05353235 0.95760884]\n",
      " [0.         0.        ]]\n",
      " epoch 30\n",
      "[[0.00173856 0.31695903]\n",
      " [0.03482628 0.52392111]\n",
      " [0.05700908 0.75559452]\n",
      " [0.05353235 0.96184796]\n",
      " [0.         0.        ]]\n",
      " epoch 31\n",
      "[[0.00173856 0.33241602]\n",
      " [0.03482628 0.5395325 ]\n",
      " [0.05700908 0.77650756]\n",
      " [0.11717324 0.96566316]\n",
      " [0.         0.        ]]\n",
      " epoch 32\n",
      "[[0.00173856 0.34773235]\n",
      " [0.03482628 0.55546493]\n",
      " [0.05700908 0.78576649]\n",
      " [0.11717324 0.96909685]\n",
      " [0.         0.        ]]\n",
      " epoch 33\n",
      "[[0.00173856 0.36295095]\n",
      " [0.03482628 0.57063742]\n",
      " [0.05700908 0.79440856]\n",
      " [0.11717324 0.97218716]\n",
      " [0.         0.        ]]\n",
      " epoch 34\n",
      "[[0.00173856 0.37801323]\n",
      " [0.03482628 0.58507045]\n",
      " [0.05700908 0.80246455]\n",
      " [0.11717324 0.97496844]\n",
      " [0.         0.        ]]\n",
      " epoch 35\n",
      "[[0.00173856 0.39286825]\n",
      " [0.03482628 0.59878522]\n",
      " [0.05700908 0.80996525]\n",
      " [0.11717324 0.9774716 ]\n",
      " [0.         0.        ]]\n",
      " epoch 36\n",
      "[[0.00173856 0.40747209]\n",
      " [0.03482628 0.61180357]\n",
      " [0.05700908 0.81694117]\n",
      " [0.11717324 0.97972444]\n",
      " [0.         0.        ]]\n",
      " epoch 37\n",
      "[[0.00173856 0.4217872 ]\n",
      " [0.03482628 0.62414792]\n",
      " [0.05700908 0.82342225]\n",
      " [0.11717324 0.981752  ]\n",
      " [0.         0.        ]]\n",
      " epoch 38\n",
      "[[0.00173856 0.43578179]\n",
      " [0.03482628 0.63584113]\n",
      " [0.05700908 0.82943771]\n",
      " [0.11717324 0.9835768 ]\n",
      " [0.         0.        ]]\n",
      " epoch 39\n",
      "[[0.00173856 0.44942932]\n",
      " [0.03482628 0.64690641]\n",
      " [0.05700908 0.83501585]\n",
      " [0.11717324 0.98521912]\n",
      " [0.         0.        ]]\n",
      " epoch 40\n",
      "[[0.00173856 0.46270796]\n",
      " [0.03482628 0.65736719]\n",
      " [0.05700908 0.84018398]\n",
      " [0.11717324 0.98669721]\n",
      " [0.         0.        ]]\n",
      " epoch 41\n",
      "[[0.00173856 0.47560021]\n",
      " [0.03482628 0.66724703]\n",
      " [0.05700908 0.84496833]\n",
      " [0.11717324 0.98802748]\n",
      " [0.         0.        ]]\n",
      " epoch 42\n",
      "[[0.00173856 0.48809242]\n",
      " [0.03482628 0.67656948]\n",
      " [0.05700908 0.84939397]\n",
      " [0.11717324 0.98922474]\n",
      " [0.         0.        ]]\n",
      " epoch 43\n",
      "[[0.00173856 0.50017443]\n",
      " [0.03482628 0.68535799]\n",
      " [0.05700908 0.8534848 ]\n",
      " [0.11717324 0.99030226]\n",
      " [0.         0.        ]]\n",
      " epoch 44\n",
      "[[0.00173856 0.51183921]\n",
      " [0.03482628 0.69363582]\n",
      " [0.05700908 0.85726353]\n",
      " [0.11717324 0.99127204]\n",
      " [0.         0.        ]]\n",
      " epoch 45\n",
      "[[0.00173856 0.52308251]\n",
      " [0.03482628 0.70142596]\n",
      " [0.05700908 0.86075166]\n",
      " [0.11717324 0.99214483]\n",
      " [0.         0.        ]]\n",
      " epoch 46\n",
      "[[0.00173856 0.5339026 ]\n",
      " [0.03482628 0.70875101]\n",
      " [0.05700908 0.86396953]\n",
      " [0.11717324 0.99293035]\n",
      " [0.         0.        ]]\n",
      " epoch 47\n",
      "[[0.04961594 0.54429993]\n",
      " [0.03482628 0.71563317]\n",
      " [0.05700908 0.86693631]\n",
      " [0.11717324 0.99363731]\n",
      " [0.         0.        ]]\n",
      " epoch 48\n",
      "[[0.09364134 0.55427692]\n",
      " [0.03482628 0.72209412]\n",
      " [0.05700908 0.86967003]\n",
      " [0.11717324 0.99427358]\n",
      " [0.         0.        ]]\n",
      " epoch 49\n",
      "[[0.09364134 0.5638377 ]\n",
      " [0.03482628 0.72815501]\n",
      " [0.05700908 0.87218765]\n",
      " [0.11717324 0.99484622]\n",
      " [0.         0.        ]]\n",
      " epoch 50\n",
      "[[0.09364134 0.57298788]\n",
      " [0.03482628 0.7338364 ]\n",
      " [0.05700908 0.87450505]\n",
      " [0.11717324 0.9953616 ]\n",
      " [0.         0.        ]]\n",
      " epoch 51\n",
      "[[0.09364134 0.58173437]\n",
      " [0.03482628 0.73915821]\n",
      " [0.05700908 0.87663709]\n",
      " [0.11717324 0.99582544]\n",
      " [0.         0.        ]]\n",
      " epoch 52\n",
      "[[0.09364134 0.59008517]\n",
      " [0.03482628 0.74413973]\n",
      " [0.05700908 0.87859767]\n",
      " [0.11717324 0.9962429 ]\n",
      " [0.         0.        ]]\n",
      " epoch 53\n",
      "[[0.09364134 0.59804923]\n",
      " [0.03482628 0.74879955]\n",
      " [0.05700908 0.88039976]\n",
      " [0.11717324 0.99661861]\n",
      " [0.         0.        ]]\n",
      " epoch 54\n",
      "[[0.09364134 0.60563627]\n",
      " [0.03482628 0.75315557]\n",
      " [0.05700908 0.88205546]\n",
      " [0.11717324 0.99695675]\n",
      " [0.         0.        ]]\n",
      " epoch 55\n",
      "[[0.09364134 0.61285664]\n",
      " [0.03482628 0.757225  ]\n",
      " [0.05700908 0.88357602]\n",
      " [0.11717324 0.99726107]\n",
      " [0.         0.        ]]\n",
      " epoch 56\n",
      "[[0.09364134 0.61972123]\n",
      " [0.03482628 0.76102435]\n",
      " [0.05700908 0.88497192]\n",
      " [0.11717324 0.99753497]\n",
      " [0.         0.        ]]\n",
      " epoch 57\n",
      "[[0.09364134 0.62624129]\n",
      " [0.03482628 0.76456938]\n",
      " [0.05700908 0.88625287]\n",
      " [0.11717324 0.99778147]\n",
      " [0.         0.        ]]\n",
      " epoch 58\n",
      "[[0.09364134 0.63242841]\n",
      " [0.03482628 0.7678752 ]\n",
      " [0.05700908 0.88742792]\n",
      " [0.11717324 0.99800332]\n",
      " [0.         0.        ]]\n",
      " epoch 59\n",
      "[[0.09364134 0.63829434]\n",
      " [0.03482628 0.7709562 ]\n",
      " [0.05700908 0.88850542]\n",
      " [0.11717324 0.99820299]\n",
      " [0.         0.        ]]\n",
      " epoch 60\n",
      "[[0.09364134 0.64385096]\n",
      " [0.03482628 0.77382606]\n",
      " [0.05700908 0.88949315]\n",
      " [0.11717324 0.99838269]\n",
      " [0.         0.        ]]\n",
      " epoch 61\n",
      "[[0.09364134 0.64911021]\n",
      " [0.03482628 0.77649784]\n",
      " [0.05700908 0.89039828]\n",
      " [0.11717324 0.99854442]\n",
      " [0.         0.        ]]\n",
      " epoch 62\n",
      "[[0.09364134 0.654084  ]\n",
      " [0.03482628 0.7789839 ]\n",
      " [0.05700908 0.89122745]\n",
      " [0.11717324 0.99868998]\n",
      " [0.         0.        ]]\n",
      " epoch 63\n",
      "[[0.09364134 0.65878415]\n",
      " [0.03482628 0.78129598]\n",
      " [0.05700908 0.8919868 ]\n",
      " [0.11717324 0.99882098]\n",
      " [0.         0.        ]]\n",
      " epoch 64\n",
      "[[0.09364134 0.66322237]\n",
      " [0.03482628 0.7834452 ]\n",
      " [0.05700908 0.8933077 ]\n",
      " [0.18579729 0.99893888]\n",
      " [0.         0.        ]]\n",
      " epoch 65\n",
      "[[0.09364134 0.6674102 ]\n",
      " [0.03482628 0.78549837]\n",
      " [0.05700908 0.89388143]\n",
      " [0.18579729 0.999045  ]\n",
      " [0.         0.        ]]\n",
      " epoch 66\n",
      "[[0.09364134 0.67136403]\n",
      " [0.03482628 0.78739786]\n",
      " [0.05700908 0.89440733]\n",
      " [0.18579729 0.9991405 ]\n",
      " [0.         0.        ]]\n",
      " epoch 67\n",
      "[[0.09364134 0.67509344]\n",
      " [0.03482628 0.79073592]\n",
      " [0.1223321  0.89488924]\n",
      " [0.18579729 0.99922645]\n",
      " [0.         0.        ]]\n",
      " epoch 68\n",
      "[[0.09364134 0.67875033]\n",
      " [0.03482628 0.79220236]\n",
      " [0.1223321  0.8953307 ]\n",
      " [0.18579729 0.9993038 ]\n",
      " [0.         0.        ]]\n",
      " epoch 69\n",
      "[[0.09364134 0.68217351]\n",
      " [0.03482628 0.79356189]\n",
      " [0.1223321  0.89573497]\n",
      " [0.18579729 0.99937342]\n",
      " [0.         0.        ]]\n",
      " epoch 70\n",
      "[[0.09364134 0.68537673]\n",
      " [0.03482628 0.79482185]\n",
      " [0.1223321  0.89610508]\n",
      " [0.18579729 0.99943608]\n",
      " [0.         0.        ]]\n",
      " epoch 71\n",
      "[[0.09364134 0.68837302]\n",
      " [0.03482628 0.79598912]\n",
      " [0.1223321  0.89644382]\n",
      " [0.18579729 0.99949247]\n",
      " [0.         0.        ]]\n",
      " epoch 72\n",
      "[[0.09364134 0.69117474]\n",
      " [0.03482628 0.79707015]\n",
      " [0.1223321  0.89675376]\n",
      " [0.18579729 0.99954322]\n",
      " [0.         0.        ]]\n",
      " epoch 73\n",
      "[[0.09364134 0.69379358]\n",
      " [0.03482628 0.79807097]\n",
      " [0.1223321  0.89703728]\n",
      " [0.18579729 0.9995889 ]\n",
      " [0.         0.        ]]\n",
      " epoch 74\n",
      "[[0.09364134 0.69624061]\n",
      " [0.03482628 0.79899723]\n",
      " [0.1223321  0.89729655]\n",
      " [0.18579729 0.99963001]\n",
      " [0.         0.        ]]\n",
      " epoch 75\n",
      "[[0.09364134 0.6985263 ]\n",
      " [0.03482628 0.7998542 ]\n",
      " [0.1223321  0.8975336 ]\n",
      " [0.18579729 0.99966701]\n",
      " [0.         0.        ]]\n",
      " epoch 76\n",
      "[[0.09364134 0.70066055]\n",
      " [0.03482628 0.8006468 ]\n",
      " [0.1223321  0.89775027]\n",
      " [0.18579729 0.99970031]\n",
      " [0.         0.        ]]\n",
      " epoch 77\n",
      "[[0.09364134 0.7026527 ]\n",
      " [0.03482628 0.80137965]\n",
      " [0.1223321  0.89794827]\n",
      " [0.18579729 0.99973028]\n",
      " [0.         0.        ]]\n",
      " epoch 78\n",
      "[[0.09364134 0.7045116 ]\n",
      " [0.03482628 0.80205703]\n",
      " [0.1223321  0.89812917]\n",
      " [0.18579729 0.99975725]\n",
      " [0.         0.        ]]\n",
      " epoch 79\n",
      "[[0.09364134 0.70624557]\n",
      " [0.03482628 0.80268295]\n",
      " [0.1223321  0.8982944 ]\n",
      " [0.18579729 0.99978153]\n",
      " [0.         0.        ]]\n",
      " epoch 80\n",
      "[[0.09364134 0.70786248]\n",
      " [0.03482628 0.80326115]\n",
      " [0.1223321  0.8984453 ]\n",
      " [0.18579729 0.99980337]\n",
      " [0.         0.        ]]\n",
      " epoch 81\n",
      "[[0.09364134 0.70936974]\n",
      " [0.03482628 0.80379511]\n",
      " [0.1223321  0.89858307]\n",
      " [0.18579729 0.99982304]\n",
      " [0.         0.        ]]\n",
      " epoch 82\n",
      "[[0.09364134 0.71077432]\n",
      " [0.03482628 0.80428808]\n",
      " [0.1223321  0.89870884]\n",
      " [0.18579729 0.99984073]\n",
      " [0.         0.        ]]\n",
      " epoch 83\n",
      "[[0.09364134 0.71208282]\n",
      " [0.03482628 0.80474306]\n",
      " [0.1223321  0.89882362]\n",
      " [0.18579729 0.99985666]\n",
      " [0.         0.        ]]\n",
      " epoch 84\n",
      "[[0.09364134 0.71330141]\n",
      " [0.03482628 0.80516288]\n",
      " [0.1223321  0.89902262]\n",
      " [0.24812112 0.99987099]\n",
      " [0.         0.        ]]\n",
      " epoch 85\n",
      "[[0.09364134 0.71443593]\n",
      " [0.03482628 0.80555863]\n",
      " [0.1223321  0.89910875]\n",
      " [0.24812112 0.99988389]\n",
      " [0.         0.        ]]\n",
      " epoch 86\n",
      "[[0.09364134 0.71644363]\n",
      " [0.09573799 0.80592256]\n",
      " [0.1223321  0.89918742]\n",
      " [0.24812112 0.9998955 ]\n",
      " [0.         0.        ]]\n",
      " epoch 87\n",
      "[[0.09364134 0.7173323 ]\n",
      " [0.09573799 0.80625717]\n",
      " [0.1223321  0.89925928]\n",
      " [0.24812112 0.99990595]\n",
      " [0.         0.        ]]\n",
      " epoch 88\n",
      "[[0.09364134 0.71816221]\n",
      " [0.09573799 0.80656479]\n",
      " [0.1223321  0.89932489]\n",
      " [0.24812112 0.99991536]\n",
      " [0.         0.        ]]\n",
      " epoch 89\n",
      "[[0.09364134 0.71893682]\n",
      " [0.09573799 0.80684755]\n",
      " [0.1223321  0.89938478]\n",
      " [0.24812112 0.99992382]\n",
      " [0.         0.        ]]\n",
      " epoch 90\n",
      "[[0.09364134 0.71965942]\n",
      " [0.09573799 0.80710742]\n",
      " [0.1223321  0.89943945]\n",
      " [0.24812112 0.99993144]\n",
      " [0.         0.        ]]\n",
      " epoch 91\n",
      "[[0.09364134 0.72033315]\n",
      " [0.09573799 0.80734623]\n",
      " [0.1223321  0.89948933]\n",
      " [0.24812112 0.9999383 ]\n",
      " [0.         0.        ]]\n",
      " epoch 92\n",
      "[[0.09364134 0.72096099]\n",
      " [0.09573799 0.80756565]\n",
      " [0.1223321  0.89953484]\n",
      " [0.24812112 0.99994447]\n",
      " [0.         0.        ]]\n",
      " epoch 93\n",
      "[[0.09364134 0.7215458 ]\n",
      " [0.09573799 0.80776722]\n",
      " [0.1223321  0.89957636]\n",
      " [0.24812112 0.99995002]\n",
      " [0.         0.        ]]\n",
      " epoch 94\n",
      "[[0.09364134 0.72258029]\n",
      " [0.15115231 0.80795237]\n",
      " [0.1223321  0.89961423]\n",
      " [0.24812112 0.99995502]\n",
      " [0.         0.        ]]\n",
      " epoch 95\n",
      "[[0.09364134 0.72303798]\n",
      " [0.15115231 0.80812241]\n",
      " [0.1223321  0.89964876]\n",
      " [0.24812112 0.99995952]\n",
      " [0.         0.        ]]\n",
      " epoch 96\n",
      "[[0.09364134 0.7234652 ]\n",
      " [0.15115231 0.80827856]\n",
      " [0.1223321  0.89968024]\n",
      " [0.24812112 0.99996356]\n",
      " [0.         0.        ]]\n",
      " epoch 97\n",
      "[[0.09364134 0.72386375]\n",
      " [0.15115231 0.80855095]\n",
      " [0.18285686 0.89970893]\n",
      " [0.24812112 0.99996721]\n",
      " [0.         0.        ]]\n",
      " epoch 98\n",
      "[[0.09364134 0.72424696]\n",
      " [0.15115231 0.80866966]\n",
      " [0.18285686 0.89973509]\n",
      " [0.24812112 0.99997049]\n",
      " [0.         0.        ]]\n",
      " epoch 99\n",
      "[[0.09364134 0.72460253]\n",
      " [0.15115231 0.80877885]\n",
      " [0.18285686 0.89975892]\n",
      " [0.24812112 0.99997344]\n",
      " [0.         0.        ]]\n",
      "L'agent se déplace vers la droite, nouvelle position: 1\n",
      "L'agent se déplace vers la droite, nouvelle position: 2\n",
      "L'agent se déplace vers la droite, nouvelle position: 3\n",
      "L'agent se déplace vers la droite, nouvelle position: 4\n"
     ]
    }
   ],
   "source": [
    "# Paramètres de l'algorithme Q-learning\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.9  # Facteur d'actualisation\n",
    "epsilon = 0.1  # Taux d'exploration\n",
    "num_episodes = 100  # Nombre total d'épisodes pour l'entraînement \n",
    "\n",
    "# Initialisation de la table Q\n",
    "num_states = 5\n",
    "num_actions = 2  # 0: gauche, 1: droite\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Fonction pour choisir une action\n",
    "def choose_action(state):\n",
    "    choice = np.random.rand()\n",
    "    if choice < epsilon:\n",
    "        action = np.random.choice([i for i in range(num_actions)])\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    return action\n",
    "\n",
    "# Fonction pour déterminer le nouvel état suite à l'action\n",
    "def get_next_state(action , state):\n",
    "    if action == 1:\n",
    "        step = 1\n",
    "    else:\n",
    "        step = -1\n",
    "    \n",
    "    new_state = state + step\n",
    "    if new_state >4 or new_state< 0:\n",
    "        new_state = state\n",
    "    return new_state\n",
    "\n",
    "# Fonction pour obtenir une récompense\n",
    "def get_reward(next_state):\n",
    "    return 1 if next_state == 4 else 0\n",
    "\n",
    "# Fonction pour mettre à jour la table Q\n",
    "def update_Q(state, action, reward, next_state):\n",
    "    Q[state,action] = Q[state,action] + alpha * (reward + gamma * np.amax(Q[next_state]) - Q[state,action])\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for episode in range(num_episodes):\n",
    "    state = 0  # On commence toujours à la position 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state = get_next_state(action , state)\n",
    "        reward = get_reward(next_state)\n",
    "        update_Q(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        if state == num_states - 1:\n",
    "            done = True  # L'objectif est atteint\n",
    "# Tester l'agent après l'entraînement\n",
    "state = 0\n",
    "\n",
    "\n",
    "while state != num_states - 1:\n",
    "    action = np.argmax(Q[state])\n",
    "    state += 1 if action == 1 else -1\n",
    "    print(f\"L'agent se déplace vers {'la droite' if action == 1 else 'la gauche'}, nouvelle position: {state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6676b5a",
   "metadata": {},
   "source": [
    "## 2. Application à la simulation de navigation d'un robot\n",
    "\n",
    "Dans cet exercice plus avancé, nous allons simuler un robot qui doit naviguer dans un environnement afin d'atteindre un objectif tout en évitant des obstacles et des zones radioactives.\n",
    "\n",
    "### Contexte\n",
    "Nous supposons que le robot est équipé d'un capteur unifié permettant à la fois de détecter les obstacles, les zones radioactives et les limites de l'environnement. L'objectif est de naviguer de son point de départ à un point d'arrivée spécifié sans entrer en collision avec les obstacles et en évitant les zones radioactives.\n",
    "\n",
    "### Environnement\n",
    "L'environnement est une grille de taille 5$\\times$5, avec des obstacles statiques et des zones irradiées. Le robot peut recevoir des informations de son capteur indiquant la distance aux obstacles et aux zones radioactives les plus proches dans chaque direction.\n",
    "\n",
    "### Règles\n",
    "- Le robot peut se déplacer dans quatre directions: haut, bas, gauche, droite.\n",
    "- Les obstacles et les zones radioactives sont fixés aléatoirement au départ et ne changent pas de position en cours de simulation.\n",
    "- Si le robot entre en collision avec un obstacle, une pénalité de -50 est attribuée.\n",
    "- Si le robot pénètre dans une zone radioactive, une pénalité de -100 est attribuée.\n",
    "- Atteindre l'objectif donne une récompense de +100.\n",
    "- Le capteur a une portée limitée et ne peut détecter des obstacles et des zones radioactives que dans un certain rayon.\n",
    "\n",
    "### Implémentation avec des classes\n",
    "Nous allons définir plusieurs classes pour représenter l'environnement, le robot et le capteur :\n",
    "\n",
    "- Classe `Environment` : Cette classe représente la grille et gére les positions des obstacles, des zones radioactives et de l'objectif à atteindre.\n",
    "- Classe `Robot` : Cette classe représente notre agent et possède une méthode pour choisir une action basée sur les entrées de son capteur et une méthode pour mettre à jour sa position dans l'environnement.\n",
    "- Classe `Sensor` : Cette classe simule le capteur du robot et fournit des informations sur son environnement.\n",
    "- Classe `Simulation` : Cette classe orchestre l'interaction entre le robot et l'environnement, et permet de visualiser les résultats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717dd38-bdd7-4713-be9a-ac8bc2caeb02",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 183, 77, 0.6); padding:5pt\" >    \n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercice n° 2.1 : Création de la classe Environment**\n",
    "\n",
    "**1. Initialisation de l'environnement** :\n",
    "\n",
    "La classe `Environment` est munie des attributs `size` (taille d'un côté de la grille carrée), `obstacles` (un ensemble / `set` contenant les obstacles), `radioactive_zones` (un ensemble / `set` contenant les zones radioactives) et `target_position` (la position de la cible). \n",
    "Les obstacles et les zones radioactives doivent être placés aléatoirement dans la grille, en veillant à ce qu'ils ne se trouvent pas sur la position de départ du robot (0, 0) ni sur la position de la cible à atteindre.\n",
    "Le contructeur prend les paramètres `size`, `obstacle_count` (nombre d'obstacles), `radioactive_count` (nombre de zones radioactives) et `target_position`, et il appelle les méthodes privées `_generate_obstacles()` et `_generate_radioactive_zones()` afin de peupler l'environnement avec le nombre spécifié d'obstacles et de zones radioactives. \n",
    "\n",
    "**2. Génération des obstacles et des zones radioactives** :\n",
    "\n",
    "Implémentez les méthodes privées `_generate_obstacles()` et `_generate_radioactive_zones()` pour peupler l'environnement avec le nombre spécifié d'obstacles et de zones radioactives. Ces méthodes **sont appelées depuis le constructeur**. Assurez-vous que les obstacles et les zones radioactives ne se chevauchent pas et ne sont pas placés sur la position initiale ou la position de la cible.\n",
    "\n",
    "**3. Détection des bords, des obstacles et des zones radioactives** :\n",
    "\n",
    "La méthode `is_edge()` doit déterminer si une position donnée est sur le bord de la grille.\n",
    "Les méthodes `is_obstacle()` et `is_radioactive()` doivent vérifier si une position donnée contient un obstacle ou une zone radioactive, respectivement.\n",
    "\n",
    "**4. Affichage de l'environnement** :\n",
    "\n",
    "La méthode `display()` doit créer et retourner une représentation sous forme de grille de l'environnement, où les cases vides sont marquées par 0, les obstacles par -1, les zones radioactives par -2, et la position du but par 1.\n",
    "\n",
    "**5. Test de la classe** :\n",
    "\n",
    "Testez la classe `Environment` à l'aide du code fourni plus bas.\n",
    "\n",
    "**Consignes supplémentaires** :\n",
    "\n",
    "Utilisez la bibliothèque *Numpy* pour générer des nombres aléatoires lors de la création des obstacles et des zones radioactives.\n",
    "Veillez à ce que la position de la cible soit toujours accessible et ne soit pas bloquée par un obstacle ou une zone radioactive.\n",
    "Assurez-vous que la méthode `display()` renvoie une grille qui reflète correctement l'état actuel de l'environnement. \n",
    "\n",
    "---\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce07d3b5-4d79-413a-abb3-4661e76c27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, size, obstacle_count, radioactive_count, target_position):\n",
    "        pass\n",
    "\n",
    "    def _generate_obstacles(self, obstacle_count):\n",
    "        pass\n",
    "\n",
    "    def _generate_radioactive_zones(self, radioactive_count):\n",
    "        pass\n",
    "\n",
    "    def is_edge(self, position):\n",
    "        pass\n",
    "\n",
    "    def is_obstacle(self, position):\n",
    "        pass\n",
    "\n",
    "    def is_radioactive(self, position):\n",
    "        pass\n",
    "\n",
    "    def get_target_position(self):\n",
    "        pass\n",
    "\n",
    "    def display(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64b7a7-3880-4bb9-a0fe-8dacd1c38c10",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e566a8b6-ea0a-43df-9ee4-0326f2d4e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, -2, 0, 0], [-1, 0, 0, 0, 0], [0, 0, -2, -1, 0], [0, -1, 0, 0, 0], [0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "size = 5 # grille de 5x5\n",
    "obstacle_count = 3\n",
    "radioactive_count = 2\n",
    "target_position = (size - 1, size - 1)\n",
    "\n",
    "env = Environment(size, obstacle_count, radioactive_count, target_position)\n",
    "print(env.display())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f958825-a153-4012-9948-a3222e3b77e7",
   "metadata": {},
   "source": [
    "#### Test de la classe `Environment`\n",
    "\n",
    "Assurez-vous que la classe `Environment` est définie dans une des cellules précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "922ea1f0-c3a4-4793-84c3-581f99de3265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les tests pour la classe Environment ont réussi.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de tests pour la classe Environment.\n",
    "\n",
    "# Création de l'environnement avec des paramètres spécifiques\n",
    "size = 5\n",
    "obstacle_count = 2\n",
    "radioactive_count = 2\n",
    "target_position = (4, 4)\n",
    "env = Environment(size, obstacle_count, radioactive_count, target_position)\n",
    "\n",
    "# Vérifier que la taille de l'environnement est correcte\n",
    "assert env.size == size, f\"Taille attendue: {size}, obtenue: {env.size}\"\n",
    "\n",
    "# Vérifier que le nombre d'obstacles est correct\n",
    "assert len(env.obstacles_count) == obstacle_count, f\"Nombre d'obstacles attendu: {obstacle_count}, obtenu: {len(env.obstacles)}\"\n",
    "\n",
    "# Vérifier que le nombre de zones radioactives est correct\n",
    "assert len(env.radioactive_zones) == radioactive_count, f\"Nombre de zones radioactives attendu: {radioactive_count}, obtenu: {len(env.radioactive_zones)}\"\n",
    "\n",
    "# Vérifier que la position de l'objectif est correcte\n",
    "assert env.target_position == target_position, f\"Position de l'objectif attendue: {target_position}, obtenue: {env.target_position}\"\n",
    "\n",
    "# Vérifier que la méthode display renvoie une matrice de la bonne taille\n",
    "display = env.display()\n",
    "assert isinstance(display, list), \"La méthode display doit retourner une liste.\"\n",
    "assert all(isinstance(row, list) for row in display), \"Chaque élément de la liste renvoyée par display doit être une liste.\"\n",
    "assert all(len(row) == size for row in display), \"Chaque sous-liste renvoyée par display doit avoir une longueur égale à size.\"\n",
    "\n",
    "# Vérifier que les coins de la grille font partie de la bordure\n",
    "corners = [(0, 0), (0, 4), (4, 0), (4, 4)]\n",
    "for corner in corners:\n",
    "    assert env.is_edge(corner), f\"Le coin en position {corner} devrait être détecté comme bordure.\"\n",
    "\n",
    "# Le centre de la grille ne peut pas faire partie de la bordure dans notre cas\n",
    "center = (2, 2)\n",
    "assert not env.is_edge(center), f\"Le centre {center} ne devrait pas être détecté comme bordure.\"\n",
    "\n",
    "# Vérifier que les bords sont détectés\n",
    "edges = [(0, 1), (0, 2), (0, 3), (1, 0), (1, 4), (2, 0), (2, 4), (3, 0), (3, 4), (4, 1), (4, 2), (4, 3)]\n",
    "for edge in edges:\n",
    "    assert env.is_edge(edge), f\"La cellule en position {corner} devrait être détecté comme bordure.\"\n",
    "\n",
    "# Vérifier que les obstacles et les zones radioactives sont correctement placés dans la matrice renvoyée par display\n",
    "for y, row in enumerate(display):\n",
    "    for x, cell in enumerate(row):\n",
    "        if (x, y) in env.obstacles:\n",
    "            assert cell == -1, f\"Obstacle attendu en position {(x, y)}, mais obtenu {cell}\"\n",
    "        elif (x, y) in env.radioactive_zones:\n",
    "            assert cell == -2, f\"Zone radioactive attendue en position {(x, y)}, mais obtenu {cell}\"\n",
    "        elif (x, y) == env.target_position:\n",
    "            assert cell == 1, f\"Objectif attendu en position {(x, y)}, mais obtenu {cell}\"\n",
    "        else:\n",
    "            assert cell == 0, f\"Case vide attendue en position {(x, y)}, mais obtenu {cell}\"\n",
    "\n",
    "print(\"Tous les tests pour la classe Environment ont réussi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6426c7-93cc-45e8-ad30-54b6480da6ca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 183, 77, 0.6); padding:5pt\" >    \n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercice n° 2.2 : Création de la classe Sensor**\n",
    "\n",
    "Vous allez développer une classe `Sensor` qui simule le fonctionnement d'un capteur pour un robot. Ce capteur permet de détecter des obstacles, des zones radioactives et les bords de l'environnement dans lequel le robot se déplace.\n",
    "\n",
    "**1. Initialisation du capteur** :\n",
    "\n",
    "La classe `Sensor` doit être initialisée avec un paramètre `range_detection` qui définit la portée de détection du capteur.\n",
    "La portée de détection est la distance maximale à laquelle le capteur peut détecter un élément dans l'environnement.\n",
    "\n",
    "**2. Détection dans l'environnement** :\n",
    "\n",
    "Implémentez la méthode `detect()` qui prend en paramètres une position (sous forme de tuple de coordonnées) et un environment (instance de la classe `Environment`). La méthode doit retourner un dictionnaire appelé `readings` dont les clés `'up'`, `'down'`, `'left'`, et `'right'` correspondent aux directions dans lesquelles le capteur effectue ses détections, et les valeurs correspondent au type de case :\n",
    "\n",
    "- Si un obstacle est détecté dans une direction, la valeur correspondante dans le dictionnaire `readings` doit être -1.\n",
    "- Si une zone radioactive est détectée, la valeur doit être -2.\n",
    "- Si le bord de l'environnement est détecté, la valeur doit être -3.\n",
    "- Si rien n'est détecté dans la portée du capteur pour une direction donnée, la valeur doit rester 0.\n",
    " \n",
    "Pour chaque direction, le capteur doit sonder l'environnement à partir de la position donnée, jusqu'à atteindre sa portée maximale ou rencontrer un élément à détecter.\n",
    "\n",
    "**3. Calcul de la position du capteur** :\n",
    "\n",
    "La méthode privée `_probe_position()` doit calculer et retourner la position de la case sondée par le capteur en fonction de la direction et de la distance spécifiées. Cette méthode est utilisée par `detect()` pour déterminer la position à sonder dans l'environnement.\n",
    "\n",
    "**Consignes supplémentaires** :\n",
    "\n",
    "Assurez-vous que la méthode `detect()` vérifie correctement les conditions de l'environnement en utilisant les méthodes `is_obstacle()`, `is_radioactive()`, et `is_edge()` fournies par l'instance de `Environment`.\n",
    "La détection doit s'arrêter immédiatement si un élément est détecté ou si la cellule sondée est au bord de l'environnement, quelle que soit la portée du capteur.\n",
    "La méthode `_probe_position()` est privée n'est utilisée qu'en interne par la classe `Sensor`.\n",
    "\n",
    "**Bonus** :\n",
    "\n",
    "Si vous avez terminé l'implémentation de base, essayez d'ajouter une fonctionnalité supplémentaire où le capteur peut détecter plusieurs éléments dans une direction donnée et retourner une liste des détections au lieu d'une seule valeur.\n",
    "\n",
    "\n",
    "---\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614f932f-ace1-4c12-961a-958848bacbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sensor:\n",
    "    def __init__(self, range_detection):\n",
    "        pass\n",
    "\n",
    "    def detect(self, position, environment):\n",
    "        pass\n",
    "\n",
    "    def _probe_position(self, position, direction, distance):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f64e2-0de2-4f05-a3ae-22840716d9d1",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation\n",
    "\n",
    "```python\n",
    "sensor = Sensor(range_detection=3)\n",
    "robot_position = (0, 0)  # Position hypothétique du robot\n",
    "\n",
    "# Supposons que l'environnement 'env' a été créé précédemment\n",
    "sensor_readings = sensor.detect(robot_position, env)\n",
    "print(sensor_readings)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed24285-ffed-4bfa-9769-f7aa40931c92",
   "metadata": {},
   "source": [
    "#### Test de la classe `Sensor`\n",
    "\n",
    "Assurez-vous que les classes `Environment` et `Sensor` sont définies dans les cellules précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e9fd58e-6c1f-41a6-81ca-2371ee5ac995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les tests pour la classe Sensor ont réussi.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de tests pour votre classe Sensor.\n",
    "size = 5\n",
    "obstacle_count = 2\n",
    "radioactive_count = 2\n",
    "goal_position = (4, 4)\n",
    "env = Environment(size, obstacle_count, radioactive_count, goal_position)\n",
    "\n",
    "# Création d'un capteur\n",
    "sensor = Sensor(range_detection=1)\n",
    "\n",
    "# Position de test pour le robot\n",
    "robot_position = (2, 2)\n",
    "\n",
    "# Test de la détection des obstacles et des zones radioactives\n",
    "# Nous plaçons manuellement des obstacles et des zones radioactives pour ce test\n",
    "env.obstacles = [(1, 2), (3, 2)]  # Gauche et droite du robot\n",
    "env.radioactive_zones = [(2, 1), (2, 3)]  # Haut et bas du robot\n",
    "\n",
    "# Détecter les obstacles et les zones radioactives\n",
    "detections = sensor.detect(robot_position, env)\n",
    "\n",
    "# Vérifier que les obstacles sont correctement détectés\n",
    "expected_obstacle_detections = {'left': -1, 'right': -1, 'up': -2, 'down': -2}\n",
    "for direction, detection in expected_obstacle_detections.items():\n",
    "    assert detections[direction] == detection, f\"Obstacle mal détecté à {direction}: attendu {detection}, obtenu {detections[direction]}\"\n",
    "\n",
    "# Vérifier que les zones radioactives sont correctement détectées\n",
    "expected_radioactive_detections = {'left': 0, 'right': 0, 'up': -2, 'down': -2}\n",
    "for direction, detection in expected_radioactive_detections.items():\n",
    "    # Nous espérons des zones radioactives uniquement vers le haut et vers le bas\n",
    "    if direction in ['up', 'down']:\n",
    "        assert detections[direction] == detection, f\"Zone radioactive mal détectée à {direction}: attendu {detection}, obtenu {detections[direction]}\"\n",
    "\n",
    "# Test de la détection des limites de l'environnement\n",
    "edge_position = (1, 1)  # Position du robot près d'un bord de l'environnement\n",
    "detections = sensor.detect(edge_position, env)\n",
    "\n",
    "# Vérifier que les limites sont correctement détectées\n",
    "expected_edge_detections = {'left': -3, 'right': -2, 'up': -3, 'down': -1}\n",
    "for direction, detection in expected_edge_detections.items():\n",
    "    assert detections[direction] == detection, f\"Limite mal détectée à {direction}: attendu {detection}, obtenu {detections[direction]}\"\n",
    "\n",
    "print(\"Tous les tests pour la classe Sensor ont réussi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450e8d6-546f-4751-8152-66494b16f541",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 183, 77, 0.6); padding:5pt\" >    \n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercice n° 2.3 : Création de la classe Robot**\n",
    "\n",
    "Écrire la classe `Robot` avec les attributs et méthodes décrits ci-dessous.\n",
    "\n",
    "**Attributs** :\n",
    "\n",
    "- `actions` : Une liste des actions possibles que le robot peut prendre (`'up'`, `'down'`, `'left'`, `'right'`).\n",
    "- `Q_table` : un dictionnaire  vide qui stockera les valeurs Q pour chaque paire état-action.\n",
    "- `sensor` : Un attribut qui stockera une instance de la classe Sensor (non fournie ici, à supposer existante).\n",
    "- `position` : La position actuelle du robot dans l'environnement (initialisée à (0, 0) au départ).\n",
    "- `environment` : L'environnement dans lequel le robot évolue (doit être défini avant que le robot puisse agir).\n",
    "- `verbose` : Un booléen qui, lorsqu'il est à True, active l'affichage de détails sur les actions et les positions du robot.\n",
    "\n",
    "**Méthodes** :\n",
    "\n",
    "- `__init__()` : Initialise le robot avec une position de départ et un mode verbeux optionnel.\n",
    "- `set_environment()` : Attribue un environnement au robot.\n",
    "- `reset()` : Réinitialise la position du robot à sa position de départ.\n",
    "- `set_sensor()` : Attribue un capteur au robot.\n",
    "- `choose_action()` : Sélectionne une action à partir de l'état actuel à l'aide d'une stratégie $\\epsilon$-greedy.\n",
    "- `update_Q_table()` : Met à jour la table Q en utilisant l'équation de Bellman.\n",
    "- `take_action()` : Fait prendre une action au robot et retourne la récompense associée à cette action.\n",
    "\n",
    "**Consignes supplémentaires** :\n",
    "\n",
    "- Lorsque le robot prend une action, il doit vérifier si la nouvelle position est un obstacle, une zone radioactive, ou l'objectif. Les récompenses sont attribuées comme suit : -50 pour un obstacle, -100 pour une zone radioactive, 100 pour l'objectif, -1 si le robot tente de sortir de la grille, et 0 pour tout autre déplacement valide.\n",
    "- Si le robot tente de se déplacer en dehors de la grille, il doit rester sur place et recevoir une pénalité.\n",
    "- La méthode `take_action()` doit également gérer les cas où l'environnement n'est pas défini.\n",
    "\n",
    "---\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5388d3b9-b82e-4da9-b7ed-e69ba5d4354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self, start_position=(0, 0), verbose=False):\n",
    "        pass\n",
    "\n",
    "    def set_environment(self, environment):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_sensor(self, sensor):\n",
    "        pass\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        pass\n",
    "\n",
    "    def update_Q_table(self, old_state, action, reward, new_state, alpha, gamma):\n",
    "        pass\n",
    "\n",
    "    def take_action(self, action):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504bad5-f811-4c26-a303-4dbb72f5d740",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation\n",
    "\n",
    "```python\n",
    "env = Environment(size=5, obstacle_count=2, radioactive_count=3, target_position=(4, 4))\n",
    "robot = Robot()\n",
    "robot.set_environment(env)\n",
    "\n",
    "old_state = robot.position\n",
    "action = robot.choose_action(old_state, epsilon=0.1)\n",
    "reward = robot.take_action(action)\n",
    "new_state = robot.position\n",
    "robot.update_Q_table(old_state, action, reward, new_state, alpha=0.1, gamma=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb861-df78-4cc6-9c51-52a3b6772986",
   "metadata": {},
   "source": [
    "#### Test de la classe `Robot`\n",
    "\n",
    "Assurez-vous que les classes `Environment`, `Sensor` et `Robot` sont définies dans les cellules précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff6b1142-eed3-41e2-ac79-678491eb760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : right, ancienne position : (0, 0), nouvelle position: (0, 1)\n",
      "Obstacle à la nouvelle position : True\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : right, ancienne position : (0, 0), nouvelle position: (0, 1)\n",
      "Obstacle à la nouvelle position : True\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : right, ancienne position : (0, 1), nouvelle position: (0, 2)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : True\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Récompense : -100\n",
      "Action : right, ancienne position : (0, 2), nouvelle position: (0, 3)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : right, ancienne position : (0, 3), nouvelle position: (0, 4)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : down, ancienne position : (0, 4), nouvelle position: (1, 4)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : down, ancienne position : (1, 4), nouvelle position: (2, 4)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : down, ancienne position : (2, 4), nouvelle position: (3, 4)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Action : down, ancienne position : (3, 4), nouvelle position: (4, 4)\n",
      "Obstacle à la nouvelle position : False\n",
      "Radioactivité à la nouvelle position : False\n",
      "Bordure à la nouvelle position : True\n",
      "Position de l'objectif : (4, 4)\n",
      "Récompense : 100\n",
      "Tous les tests pour la classe Robot ont réussi.\n"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "obstacle_count = 2\n",
    "radioactive_count = 2\n",
    "target_position = (4, 4)\n",
    "env = Environment(size, obstacle_count, radioactive_count, target_position)\n",
    "\n",
    "# Création d'un robot avec son capteur unifié et associé à l'environnement\n",
    "sensor = Sensor(range_detection=1)\n",
    "robot = Robot(start_position=(0, 0), verbose=True)\n",
    "robot.set_environment(env)\n",
    "robot.set_sensor(sensor)\n",
    "\n",
    "# Test de la réinitialisation du robot\n",
    "robot.reset()\n",
    "assert robot.position == (0, 0), \"La position du robot devrait être réinitialisée à (0, 0).\"\n",
    "\n",
    "# Test de la sélection d'action epsilon-greedy\n",
    "epsilon = 0.1  # 10% de chance de choisir une action aléatoire\n",
    "action = robot.choose_action(robot.position, epsilon)\n",
    "assert action in robot.actions, \"L'action choisie doit être parmi les actions possibles.\"\n",
    "\n",
    "# Assurez-vous qu'il n'y a pas de zone radioactive à la position (0, 1)\n",
    "assert not env.is_radioactive((0, 1)), \"Il ne devrait pas y avoir de zone radioactive à la position initiale pour ce test.\"\n",
    "\n",
    "# Placez un obstacle à la position (0, 1) et vérifiez que `is_obstacle` renvoie `True`\n",
    "env.obstacles = [(0, 1)]\n",
    "assert env.is_obstacle((0, 1)), \"La position (0, 1) devrait être un obstacle.\"\n",
    "\n",
    "# Maintenant, effectuez le test de prise d'action avec l'obstacle\n",
    "reward = robot.take_action('right')\n",
    "assert reward == -50, \"La récompense devrait être -50 pour avoir frappé un obstacle.\"\n",
    "\n",
    "# Place une zone radioactive à la position (0, 2) et vérifie que is_radioactive renvoie True\n",
    "env.radioactive_zones = [(0, 2)]\n",
    "assert env.is_radioactive((0, 2)), \"La position (0, 2) devrait être radioactive.\"\n",
    "\n",
    "# Effectuons le test de prise d'action avec la zone radioactive\n",
    "robot.reset()\n",
    "reward = robot.take_action('right')\n",
    "reward = robot.take_action('right')\n",
    "print(f\"Récompense : {reward}\")\n",
    "assert reward == -100, \"La récompense devrait être -100 pour être entré dans une zone radioactive.\"\n",
    "\n",
    "# Maintenant, rendons nous sur l'objectif\n",
    "reward = robot.take_action('right')\n",
    "reward = robot.take_action('right')\n",
    "reward = robot.take_action('down')\n",
    "reward = robot.take_action('down')\n",
    "reward = robot.take_action('down')\n",
    "reward = robot.take_action('down')\n",
    "print(f\"Récompense : {reward}\")\n",
    "assert reward == 100, \"La récompense devrait être 100 pour avoir atteint l'objectif.\"\n",
    "\n",
    "print(\"Tous les tests pour la classe Robot ont réussi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71d133-1e2e-49d1-9d05-400cc62908d7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 183, 77, 0.6); padding:5pt\" >    \n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercice n° 2.4**\n",
    "\n",
    "La classe `Simulation`, vous allez intégrer plusieurs composants qui interagissent ensemble pour simuler un robot se déplaçant dans un environnement. Voici un guide étape par étape pour vous aider à comprendre et à coder cette classe.\n",
    "\n",
    "##### 1. Initialisation de la classe\n",
    "Le constructeur `__init__()` initialise le robot et son environnement, ainsi que le capteur. Il prend comme paramètres :\n",
    "\n",
    "- `robot` : une instance de la classe `Robot`.\n",
    "- `environment` : une instance de la classe `Environment`.\n",
    "- `sensor` : une instance de la classe `Sensor`.\n",
    "- `verbose` : un booléen qui, lorsqu'il est à True, active l'affichage de messages détaillés pendant la simulation. Il est à False par défaut.\n",
    "\n",
    "##### 2. Entraînement du robot\n",
    "La méthode `train()` est utilisée pour entraîner le robot à naviguer dans l'environnement. Elle prend comme paramètres :\n",
    "\n",
    "- `episodes` : le nombre total d'épisodes d'entraînement.\n",
    "- `max_steps_per_episode` : le nombre maximum de pas que le robot peut prendre dans un épisode.\n",
    "- `epsilon` : le taux d'exploration qui contrôle le compromis entre exploration et exploitation.\n",
    "- `alpha` : le taux d'apprentissage.\n",
    "- `gamma` : le facteur d'actualisation.\n",
    "\n",
    "##### 3. Trouver le meilleur chemin\n",
    "La méthode `find_best_path()` ne prend aucun paramètre. Elle réinitialise la position du robot et suit le meilleur chemin qu'il a appris jusqu'à ce qu'il atteigne l'objectif ou qu'une condition d'arrêt soit rencontrée. Cette méthode est appelée par `visualize_best_path()`.\n",
    "\n",
    "Les méthodes `visualize()` et `visualize_best_path()` sont fournies afin de vous aider à visualiser l'environnement et le chemin que le robot prend. Les cases blanches représentent les espaces libres, les cases rouges les zones radioactives, les cases grises les obstacles, et la case verte l'objectif à atteindre. Le robot est également affiché sur la grille.\n",
    "\n",
    "---\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ce909501-4117-4ed6-be74-128716af5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self, robot, environment, sensor, verbose=False):\n",
    "        # A compléter\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, max_steps_per_episode, epsilon, alpha, gamma):\n",
    "        # A compléter\n",
    "        pass\n",
    "\n",
    "    def find_best_path(self):\n",
    "        # A compléter\n",
    "        pass\n",
    "\n",
    "    def visualize(self):\n",
    "        grid = self.environment.display()\n",
    "        # Zone radioactive : rouge ; obstacle : gris ; case vide : blanc ; objectif : vert\n",
    "        cmap = colors.ListedColormap(['red', 'gray', 'white', 'green'])\n",
    "        bounds = [-2, -1, 0, 1, 2]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(grid, cmap=cmap, norm=norm, extent=[-0.5, self.environment.size-0.5, self.environment.size-0.5, -0.5])\n",
    "\n",
    "        # Ajoute du texte aux cases avec obstacle ('x'), zone radioactive ('r'), objectif ('T') et robot ('Robot')\n",
    "        for obstacle in self.environment.obstacles:\n",
    "            ax.text(obstacle[0], obstacle[1], 'x', va='center', ha='center', color='black')\n",
    "        for radioactive in self.environment.radioactive_zones:\n",
    "            ax.text(radioactive[0], radioactive[1], 'r', va='center', ha='center', color='black')\n",
    "        target = self.environment.get_target_position()\n",
    "        ax.text(target[0], target[1], 'T', va='center', ha='center', color='black')\n",
    "        robot_pos = self.robot.position\n",
    "        ax.text(robot_pos[0], robot_pos[1], 'Robot', va='center', ha='center', color='black')\n",
    "        \n",
    "        # Tracé de la grille\n",
    "        ax.set_xticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.grid(which='major', color='k', linestyle='-', linewidth=2)\n",
    "\n",
    "        # Limites pour les axes\n",
    "        ax.set_xlim(-0.5, self.environment.size-0.5)\n",
    "        ax.set_ylim(-0.5, self.environment.size-0.5)\n",
    "\n",
    "        # Suppression des labels sur les axes\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Inversion de l'axe vertical\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "    def visualize_best_path(self):\n",
    "        best_path = self.find_best_path()\n",
    "        grid = self.environment.display()\n",
    "        # Zone radioactive : rouge ; obstacle : gris ; case vide : blanc ; objectif : vert\n",
    "        cmap = colors.ListedColormap(['red', 'gray', 'white', 'green'])\n",
    "        bounds = [-2, -1, 0, 1, 2]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(grid, cmap=cmap, norm=norm, extent=[-0.5, self.environment.size-0.5, self.environment.size-0.5, -0.5])\n",
    "\n",
    "        # Annotate grid with obstacle, radioactive, goal, and robot markers\n",
    "        for obstacle in self.environment.obstacles:\n",
    "            ax.text(obstacle[0], obstacle[1], 'x', va='center', ha='center', color='black')\n",
    "        for radioactive in self.environment.radioactive_zones:\n",
    "            ax.text(radioactive[0], radioactive[1], 'r', va='center', ha='center', color='black')\n",
    "        target = self.environment.get_target_position()\n",
    "        ax.text(target[0], target[1], 'T', va='center', ha='center', color='black')\n",
    "        robot_pos = self.robot.position\n",
    "        ax.text(robot_pos[0], robot_pos[1], 'Robot', va='center', ha='center', color='black')\n",
    "\n",
    "        # Draw the best path with dotted lines\n",
    "        for (y, x) in best_path:\n",
    "            ax.plot(x, y, 'bo')  # Mark the path with blue dots\n",
    "\n",
    "        # Optionally, you can draw lines instead of dots\n",
    "        if len(best_path) > 1:\n",
    "            for i in range(len(best_path) - 1):\n",
    "                start = best_path[i]\n",
    "                end = best_path[i + 1]\n",
    "                ax.plot([start[0], end[0]], [start[1], end[1]], 'b--')  # Draw lines with blue dashed\n",
    "\n",
    "        # Set the grid lines\n",
    "        ax.set_xticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.grid(which='major', color='k', linestyle='-', linewidth=2)\n",
    "\n",
    "        # Set the axis limits\n",
    "        ax.set_xlim(-0.5, self.environment.size-0.5)\n",
    "        ax.set_ylim(-0.5, self.environment.size-0.5)\n",
    "\n",
    "        # Remove axis labels\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Invert y-axis to have the origin at the bottom left\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2476821-52a8-40dc-b95a-57f614cf0c92",
   "metadata": {},
   "source": [
    "#### Créer et entraîner une simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599b800d-a797-4775-b484-725db29c8cdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Utilisation de la classe Simulation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mEnvironment\u001b[49m(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, obstacle_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, radioactive_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, target_position\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      3\u001b[0m sensor \u001b[38;5;241m=\u001b[39m Sensor(range_detection\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m robot \u001b[38;5;241m=\u001b[39m Robot(start_position\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Environment' is not defined"
     ]
    }
   ],
   "source": [
    "# Utilisation de la classe Simulation\n",
    "env = Environment(size=5, obstacle_count=2, radioactive_count=2, target_position=(4, 4))\n",
    "sensor = Sensor(range_detection=1)\n",
    "robot = Robot(start_position=(0, 0))\n",
    "simulation = Simulation(robot, env, sensor)\n",
    "\n",
    "# Paramètres de la simulation\n",
    "episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "epsilon = 0.1  # Probabilité de choisir une action aléatoire\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.9  # Facteur d'actualisation\n",
    "\n",
    "# Visualiser l'environnement avant entraînement\n",
    "simulation.visualize()\n",
    "\n",
    "# Entraîner le robot\n",
    "simulation.train(episodes, max_steps_per_episode, epsilon, alpha, gamma)\n",
    "\n",
    "# Visualiser l'environnement après entraînement\n",
    "simulation.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d4a32-9326-4664-b327-4174e82576d9",
   "metadata": {},
   "source": [
    "#### Trouver et visualiser le meilleur chemin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3c510211-2c42-412e-aa00-b1f081e6fded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAGKCAYAAAASfgYQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcDUlEQVR4nO3df2wc553f8fcuba/4Q1RypmwtTYpScsoP6g5tYylpG1CSca7OUv9QRas9NHVwaJFDGzQXufAfRVo0uvxzQdszTjoU6bU94HIIigY5gb6kkO1ce6lk9pI0coO2qdg2rGtRIrmGpToixV1mU3OnfwypJanhsytb9Oxy3i/ggWZmH66+fqzdzzzzzC5zURRFSJK0gXzaBUiSWptBIUkKMigkSUEGhSQpyKCQJAUZFJKkIINCkhRkUEiSgh5otmO1WqVard7Zr9VqvPXWWzz88MPkcrlNKU6StDmiKOL27dv09/eTz4fnDE0HxZe//GW+9KUvveviJEmt4/r16wwMDAT75Jr9Co/1M4q5uTl2794NQLFYfBdlbm1vvPEGURSRy+XYtWtX2uW0LMepOY5TcxynxkqlEgC3bt1ix44dwb5NzygKhQKFQuGu48VikdnZ2XssMTsGBgaYmZmhv7+f6enptMtpWY5Tcxyn5jhOjfX391MqlZpaOnAxW5IUZFBIkoIMCklSkEEhSQoyKCRJQQaFJCnIoJAkBRkUkqQgg0KSFGRQSJKCDApJUpBBIUkKMigkSUEGhSQpyKCQJAUZFJKkIINCkhRkUEiSggwKSVKQQSFJCjIoJElBBoUkKcigkCQFGRSSpCCDQpIUZFBIkoIMCklSkEEhSQp6IO0C7pelJRgfh1IJikUYGYGOjrSrkqT2tyWCYmwMTp+G6en6sYEBOHcORkfTq0uStoK2v/Q0NganTq0NCYCZmfj42Fg6dUnSVtHWM4qlpXgmEUV3P7Zy7POfhyefjC9DdXTAtm31PuXyxs+dz0Nn5zvrW6nU//5arRPoolbrpFyGXA66upL7rre+7+Ii1Gob19Hd/c76SlJIWwfF+PjdM4n1ZmZgx454+/hxuHCh/tgjj8Rv1EkOH4aLF+v7e/bAzZvJfQ8cgMuX6/vDwzA1tbI3CcRrJz098WNXrtT7HjwIExPJzzs0BFev1vcPHYJXX03u29cHN27U948dg0uXkvt2dYWDT5JWa+tLT6VS2hW0p0olnq0YFpKa0dYzimKxuX4vvhifja+/C+rNNzf+mfy6CF19Zt+o78RE/XLSvn37KJVmKRb7mZycJJdb2/fy5fClp9VeeSV8OWm1l15K7lsuw6OPNvcckgRtHhQjI/HdTTMzyW+2uVz8+NGjybfK3st1+nvpu3pdIZ9fBCrk84uJz7G6byOr10HuZ19JCmnrS08dHfEtsHD32ffK/tmzfp5Ckt6Ntg4KiD8ncf48PPbY2uMDA/FxP0chSe9OW196WjE6CidO+MlsSdoMWyIoIA6FI0fSrkKStp4tExRqTkdH/HmSlW1JasSgyJht29Z+6FCSGmn7xWxJ0uYyKCRJQQZFxpTL8YcHu7v9Cg9JzXGNIoM2+iJESUrijEKSFGRQSJKCDApJUpBBIUkKMigkSUHe9ZQx+Xz8a15XtiWpEYMiYzo71/4ucElqxHNKSVKQQSFJCjIoMqZchp074+ZXeEhqhmsUGXTzZtoVSGonzigkSUEGhSQpyKCQJAUZFJKkIINCkhTkXU8Zk8/DgQP1bUlqxKDImM5OuHw57SoktRPPKSVJQQaFJCnIoMiYSgX27IlbpZJ2NZLagWsUGRNFMDVV35akRpxRSJKCclHU3HlltVqlWq3e2Z+fn2dwcJBcLkd/f/+mFdjuSqUStVqNfD5PsVhMuxxqtU5KpUkAisV95POLKVcUa7VxalWOU3Mcp8ZmZ2eJooi5uTl6e3vDnaMmnTlzJgJsbd+6oviiU7S8nXY9NpstzTY3N9fw/f9dzygAtm/f3sxTZNLCwgJRFLXMmY0zivbmODXHcWrsXmYUTS9mFwoFCoXCXcd7enp47rnn7r3KjHj++ee5ffs2xWKR6enptMuhXIaennh7cnKS7u5061kxMDDAzMxMy4xTq3KcmuM4Ndbf30+pVGqqr3c9ZUwuB8PD9W1JasSgyJiuLrhyJe0qJLUTb4+VJAUZFJKkIIMiYyoV2L8/bn6Fh6RmuEaRMVEEExP1bUlqxBmFJCnIoJAkBRkUkqQgg0KSFGRQSJKCvOspY3I5GBqqb0tSIwZFxnR1wdWraVchqZ146UmSFGRQSJKCDIqMWVyEgwfjttgav7NIUotzjSJjajV49dX6tiQ14oxCkhRkUEiSggwKSVKQQSFJCjIoJElB3vWUQX19aVcgqZ0YFBnT3Q03bqRdhaR24qUnSVKQQSFJCjIoMmZxEY4ciZtf4SGpGa5RZEytBpcu1bclqRFnFJKkoC0TFLVajtdfH+JHP/oFXn99iFrNX9+WZGmpvv3KK2v3JSnJlrj0NDHxEV5++Snm53fcOdbbO8dTT73M8PD/TLGy1jI2Br/+6/X948dhYADOnYPR0fTqktTa2n5G8cMf9vONb5xmfv6frTr6Xebnd/KNbzzMxMRHUqutlYyNwalTMDu79vjMTHx8bCyduiS1vraeUdRqOS5e/GtAP3AS+GXgw8Cngc8Bv8TLL8/zkY/8L/L5KMVK07W0BKdPQ5QwBCvHPv95ePJJ6OiI27Zt9T7l8sbPnc9DZ+c761up1P/+Wq0T6KJW66Rchlwu/v3eSX3XW993cTG8UN/d/c76SlnV1jOKqandy5eb/jLwa8DfAP4O0A18GcgxP7+DqandKVaZvvFxmJ4O95mZgR07oKcHnn567WOPPBIfT2rHjq3tu2fPxn0PHVrbd3i4/lipNAmUKZUm6emJfwPfagcPbvy8w8Nr+x46tHHfPXvW9j12bOO+jzwSHjMpK9o6KBYWtq/a+y3gbeAPgX8NFDbolz2lUtoVtKdKJZ6thGZJUha09aWnnp7bq/ZeA2aBGnAV+MUN+mVPsdhcvxdfjM/GOzrWHn/zzY1/Jr/uVOPq1eb7TkzULyft27ePUmmWYrGfyclJcutuWrt8OXzpabVXXmn+MyIvvZTct1yGRx9t7jmkra6tg2Jo6Bq9vXPMz28DngF+hXiN4jPAj4Cd9PbOMzR0Lc0yUzcyEt/dNDOT/Gaby8WPHz16d0jAvV2nv5e+q9cV8vlFoEI+v5j4HKv7NrJ6HeR+9pWyqq0vPeXzEU899TLwD4E54HeAvw98CPhbADz11MuZXsiG+M3/3Ll4e/3Z98r+2bPJISFJbR0UAJ2dL5HLnaWr658DvcT/SV8DxvnYxz7j5yiWjY7C+fPw2GNrjw8MxMf9HIWkjbT1pSeAvXv3cubMP6JW+wFTU2+wsLCdnp7bDA09l/mZxHqjo3DiRHwXVKkUr12MjDiTkBTW9kGxIp+P2Lt3Ku0yWl5HR/zNsZLUrC0TFNL91NERf8XJyraUZQaFlGDbNrhwIe0qpNbQ9ovZkqTNZVBIkoIMCilBuRx/eLC726/wkFyjkDZQqaRdgdQanFFIkoIMCklSkEEhSQoyKCRJQQaFJCnIu56kBPk8HD5c35ayzKCQEnR2wsWLaVchtQbPlSRJQQaFJCnIoJASlMuwc2fc/AoPZZ1rFNIGbt5MuwKpNTijkCQFGRSSpCCDQpIUZFBIkoIMCklSkHc9SQnyeThwoL4tZZlBISXo7ITLl9OuQmoNnitJkoIMCklSkEEhJahUYM+euFUqaVcjpcs1CilBFMHUVH1byjJnFJKkoFwUNXe+VK1WqVard/bn5+cZHBwEYPv27ZtT3RawsLBAFEXk83mKxWLa5bSsUqlErVZrmXGq1ToplSYBKBb3kc8vplxRrNXGqVU5To3Nzs4SRRFzc3P09vaGO0dNOnPmTATYbBlpXRFEy62rBeqx2Tanzc3NNXz/f9czilwuR39/fzNPkUl3zmwAz2s2VgJq0DJngM4o2puvu8ZmiZOimRlF04vZhUKBQqFw1/Fdu3YxPT19rzVmxsDAADMzMxQBR2ljA8AMUCwWW+LfU7kMPT3x9uTkJN3d6daz4s6/pxYZp1bl666xfuITtGZ415OUIJeD4eH6tpRlBoWUoKsLrlxJuwqpNXh7rCQpyKCQJAUZFFKCSgX274+bX+GhrHONQkoQRTAxUd+WsswZhSQpyKCQJAUZFJKkIINCkhRkUEiSgrzrSUqQy8HQUH1byjKDQkrQ1QVXr6ZdhdQavPQkSQoyKCRJQQaFlGBxEQ4ejNtia/zOIik1rlFICWo1ePXV+raUZc4oJElBBoUkKcigkCQFGRSSpCCDQpIU5F1P0gb6+tKuQGoNBoWUoLsbbtxIuwqpNXjpSZIUZFBIkoIMCinB4iIcORI3v8JDWecahZSgVoNLl+rbUpY5o5AkBTmjyJgl8owzQokiRUqMME4HnjKvt7RU337lFTh6FDo60qunVS0twfg4lEpQLMLIiOOUpN1fd84oMmSMk+zhKk9wkU/xb3iCiwzxY8Y4mXZpLWVsDD760fr+8eOwZ098XHVjY/G4PPEEfOpT8Z+O0922wuvOoMiIMU5yivNM82ngc8CzQB8zfIZTnG+rf7SbaWwMTp2C2dm1x2dm4uO+CcZWxml6eu1xx2mtrfK6MygyYIk8pzlHdOfIHwAPAX8K/C4Az3KWpYz/c1hagtOnIYrufmzl2LPPrr0slUWOU3O20uuu9SvUuzbOCNMMUv/fvQ/4J8CHgQ8Tkec6uxlnJLUaW8H4+N1nyKtFEVy/HvfLMsepOVvpdWdQZECJ4rojjzfZL1tKpfvbb6tynJqzlV53BkUGFFn/iu1usl+2FJt8vTbbb6tynJqzlV53BkUGjDDOANfJbXA7Xo4ag1xjhGxfKxgZgYEByOWSH8/lYHAw7pdljlNzttLrzqDIgA5qnON04mMr/4jP8mxb3de9GTo64Ny55MdW3hTPnvVzAqvHaX1YOE51W+l1Z1BkxCgvcJ5TPER1zfEBpjnPKUZ5IaXKWsvoKJw/D/39a48PDMTHR0fTqavVrIzTY4+tPe44rbVVXnd+MjtDRnmBCnnGKVDir7flJ0TfC6Oj8OSTsGNHvP/ii34yO8noKJw44SezG9kKrzuDImM6qHGES2mX0fJWv9kdOuSb30Y6OuJv2FVYu7/uDAopQUdH/NUdK9tSlhkUUoJt2+DChbSrkFqDi9mSpCCDQpIUZFBICcpl6O6OW7mcdjVSulyjkDZQqaRdgdQanFFIkoIMCklSkEEhSQoyKCRJQQaFJCnIu56kBPk8HD5c35ayzKCQEnR2wsWLaVchtQbPlSRJQQaFJCnIoJASlMuwc2fc/AoPZZ1rFNIGbt5MuwKpNTijkCQFGRSSpCCDQpIUZFBIkoIMCklSkHc9SQnyeThwoL4tZZlBISXo7ITLl9OuQmoNnitJkoIMCklSkEEhJahUYM+euFUqaVcjpcs1CilBFMHUVH1byjJnFJKkoFwUNXe+VK1WqVard/bn5+cZHBwkl8vR39+/aQW2u1KpRK1WIw8U0y6mhZWAGpDP5ykW0x+pWq2TUmkSgGJxH/n8YsoVxe78e2qRcWpVvu4amwUiYG5ujt7e3nDnqElnzpyJlp/XZstA64ogWm5dLVCPzbY5bW5uruH7vzOKTeYZYHNabZycUbQ3x6mx2dlZoihqakbR9GJ2oVCgUCjcdXzXrl1MT0/fe5UZMTAwwMzMDMVi0XEKaLVxKpehpyfenpycpLs73XpWtNo4tSrHqbH+/n5KpVJTfb3rSUqQy8HwcH1byjKDQkrQ1QVXrqRdhdQavD1WkhRkUEiSggwKKUGlAvv3x82v8FDWuUYhJYgimJiob0tZ5oxCkhRkUEiSggwKSVKQQSFJCjIoJElB3vUkJcjlYGiovi1lmUEhJejqgqtX065Cag1eepIkBRkUkqQgg0JKsLgIBw/GbbE1fmeRlBrXKKQEtRq8+mp9W8oyZxSSpCCDQpIUZFBIkoIMCklSkEEhSQryridpA319aVcgtQaDQkrQ3Q03bqRdhdQavPQkSQoyKCRJQQaFlGBxEY4ciZtf4aGsc41CSlCrwaVL9W0py5xRSJKCnFFI0iZbWoLxcSiVoFiEkRHo6Ei7quYZFJK0icbG4PRpmJ6uHxsYgHPnYHQ0vbruhZeeJGmTjI3BqVNrQwJgZiY+PjaWTl33yqCQpE2wtBTPJKLo7sdWjj37bNyv1RkU0ga6uuImvRPj43fPJFaLIrh+Pe7X6lyjkDZQqaRdgdpZqXR/+6XJGYUkbYJi8f72S5NBIUmbYGQkvrspl0t+PJeDwcG4X6szKCRpE3R0xLfAwt1hsbJ/9mx7fJ7CoJCkTTI6CufPw2OPrT0+MBAfb5fPUbiYLUmbaHQUTpzwk9mSpICOjvibiNuVQSEl6OiA48fr21KWGRRSgm3b4MKFtKuQWoOL2ZKkIINCkhRkUEgJymXo7o5buZx2NVK6XKOQNuB3PUkxZxSSpCCDQpIUZFBIkoIMCklSkEEhSQryricpQT4Phw/Xt6UsMyikBJ2dcPFi2lVIrcFzJUlSkEEhSQoyKKQE5TLs3Bk3v8JDWecahbSBmzfTrkBqDc4oJElBBoUkKcigkCQFGRSSpCCDQpIU5F1PUoJ8Hg4cqG9LWWZQSAk6O+Hy5bSrkFqD50qSpCCDQpIUZFBICSoV2LMnbpVK2tVI6XKNQkoQRTA1Vd+WsswZhSQpKBdFzZ0vVatVqtXqnf35+XkGBwfJ5XL09/dvWoHtrlQqUavVyOfzFIvFtMtpWa02TrVaJ6XSJADF4j7y+cWUK4q12ji1KsepsdnZWaIoYm5ujt7e3nDnqElnzpyJAJstI60rgmi5dbVAPTbb5rS5ubmG7//vekYBsH379maeIpMWFhaIosgzmwZa7QzQGUV7WxknckBP2tW0qNvxH83MKJpezC4UChQKhbuO9/T08Nxzz91bgRny/PPPc/v2bYrFItPT02mX07IGBgaYmZlpmXEql6Fn+Q1mcnKS7u5061nRauPUqlbGiR7At6dkvwUsNNfVu56kBLkcDA/Xt6UsMyikBF1dcOVK2lVIrcHbYyVJQQaFJCnIoJASVCqwf3/c/AoPZZ1rFFKCKIKJifq2lGXOKCRJQQaFJCnIoJAkBRkUkqQgg0KSFORdT1KCXA6GhurbUpYZFFKCri64ejXtKqTW4KUnSVKQQSFJCjIopASLi3DwYNwWW+N3FkmpcY1CSlCrwauv1relLHNGIUkKMigkSUEGhSQpyKCQJAUZFJKkIO96kjbQ15d2BVJrMCikBN3dcONG2lVIrcFLT5KkIINCkhRkUEgJFhfhyJG4+RUeyjrXKKQEtRpculTflrLMGYUkbabXgd8A2nhm6oxCkkJeAP7r8nYe6AWGgSeABwM/9xvr9v/xuv3Dy8/RrN8G/jzwF+7hZ+4Tg0KSGvl54ARQA2aBPwJywF8K/Mxzy39+9+PwvR8A24Efx8d6ZuDnvgT8282p9z5r+6Aol8t85Stf4ROf+ASHDh0C4Nq1a3z1q1/lmWee4QMf+EDKFUpqex3E7/MAO4D/BrxGHBRvA38M/HegCvQDTwGPARMn4XufA34JWAKOAj+GhT8LL/xLePABGH4hft4J4D8AbwE9wCeAv7j8d/4+MAd8e7nB3TOWTdT2QdHd3c2JEyf4+te/zgc/+EH6+vp44YUX+PjHP25I6L4ol5OPd3TAtm2N+wHk89DZ+c76VioQRfF2rdYJdFGrdVIuQy4X/37vpL7rre+7uBheqO/ubr5vSJrjRAT8DCCCh1YtEvysk3hKkGRd3ygPUQ5+Voj336zBtZ/Cjjz8bBv8SQX+B3AS6H4IvrcEX1uCv90NL/0Od2YR/BR4HtgF/APgBLz07+Ej34Q3avCHwBFgP3AduAB0An8O+BXgd4HHgY9tPCabpe2DAuBDH/oQjz/+OGNjY/T39/Pggw/y5JNPpl2W2lxXV/zm8+ijyY8fPw4XLtT3H3kk7p/k8GG4eLG+v2cP3LyZ3PfAAbh8ub4/PAxTUyt7kwCUStDTEz925Uq978GDMDGR/LxDQ3D1an3/0KH6L2dar69v7SfTjx2r3wW23so4bSTNcWIB+E1g5xX4u79Q7/yvLsON/clPvOMq/L299f3JY7D4Mvxmjnj6UAXyUP4G/PbjUN0LfwXYB/z+t2Hqk8AeOPcsMAD87+UnKlC/VvUH8WO3/zNMjcAPL8Fe4nULgD7gBvBd4qDoIs61h6jPbN5DW+aup6NHj1Kr1bhy5QpPP/00DzywJTJQKenuDp/NKmueAP4L8J+AXwX+JvA0RFfjdYvdq/s+CHyceJqx2ur3pJ8DPhz3WSjGobB7XffdwP8lfv6UbZl307feeovbt28TRRG3bt3i0Y1OA6V7sLCw8WMdHWv333xz4775dadkq8/sG/WdmKhfUtm3bx+l0izFYj+Tk5Pk1l09uXw5fOlptVdeaf5y0ksvvbtLT6u9l+NED/B5iK9BrfJrBwleelpt30vw0xyc+jPLD0fwez+Fg1+D/jz83qq+zxyLL1Wdr8Lbu+D/bFz/HT2lJjqla0sExdtvv83Y2Bj79++nr6+Pb33rW3z2s5+lp6cn7dLU5lZfp0+r7+p1hXx+EaiQzy8mPsfqvo2svr5/P/s28l6O053LNes9dA8fasjVlp9n1fWyQ8C3fxaHUAdwDXgf8OBP4zXrEvCJWbh5HeZXEvbtVU/6E+DH0LkThsZh5/JzrHYNeJj6dZ8O7sqw98qWuPT0ne98h2q1yrFjx/jkJz/Jww8/zDe/+c20y5K0VQ0Th8dl4ADw74iXRd4EvgX8P+DxCJ46veqHqsCfEN8e9atAHxz/Y8jX4s9GvA5cAm4SX+X6AfW7niAOoilgHniPL4u2fVC8/vrrfP/73+fkyZNs27aNfD7PyZMnuXbtGpdXr3RJ0v3SQbwM8afEd75+lPiDef+C+PbWTxPfsTT8Ahw5s/xD24DTwOOQvw5Hfx5+cfmEth/4q8QZ8hXi22SfIF7IXvEEcAs4B/zTzftPS9L2l5727t3LF7/4xTXH3v/+9/OFL3whpYokbSknNzg+stwAji+3JEf+IxzKw9TBeOG6py++3JRft+gzvNw2Mgh8tvmy76e2DwpJann5Guzd4P7iNtD2l54kSZvLoJAkBRkUkqQgg0KSFGRQSJKCDApJUpBBIUkKMigkSUEGhSQpyKCQJAUZFJKkIINCkhRkUEiSggwKSVKQQSFJCjIoJElBBoUkKcigkCQFGRSSpCCDQpIUZFBIkoIMCklSkEEhSQoyKCRJQQaFJCnIoJAkBRkUkqQgg0KSFGRQSJKCclEURc10rFarVKvVO/tzc3Ps3r0bgJ6ens2pbgtYWFgAIJfLsWvXrpSraV1vvPEGURQ5Tg04Ts1ZGScAfHtKFr81cevWLXbs2BHuGzXpzJkzEWCz2Wy2LdRee+21hu//73hGcevWLYaGhrh27VrjNMqw+fl5BgcHuX79Or29vWmX07Icp+Y4Ts1xnBpbuSr0k5/8hPe9733Bvg80+6SFQoFCoXDX8R07dvg/ogm9vb2OUxMcp+Y4Ts1xnBrL5xsvVbuYLUkKMigkSUHvOCgKhQJnzpxJvBylOsepOY5Tcxyn5jhOjd3LGDW9mC1JyiYvPUmSggwKSVKQQSFJCjIoJElBBoUkKcigkCQFGRSSpCCDQpIU9P8B1IrO+22wqTUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simulation.visualize_best_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71758cd7-f493-47fd-ab0e-c3c3ff74e26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
